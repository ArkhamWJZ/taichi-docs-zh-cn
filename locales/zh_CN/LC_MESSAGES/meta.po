# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018, Yuanming Hu
# This file is distributed under the same license as the taichi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
msgid ""
msgstr ""
"Project-Id-Version: taichi 0.5.14\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-08-02 22:06+0800\n"
"PO-Revision-Date: 2020-05-13 22:03-0400\n"
"Last-Translator: \n"
"Language: zh_CN\n"
"Language-Team: \n"
"Plural-Forms: nplurals=1; plural=0\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../meta.rst:4
msgid "Metaprogramming"
msgstr "元编程"

#: ../../meta.rst:6
msgid "Taichi provides metaprogramming infrastructures. Metaprogramming can"
msgstr "Taichi为元编程提供了基础架构。元编程可以"

#: ../../meta.rst:8
msgid ""
"Unify the development of dimensionality-dependent code, such as 2D/3D "
"physical simulations"
msgstr "统一对维度依赖的代码开发，例如2维/3维（2D/3D）物理仿真"

#: ../../meta.rst:9
msgid "Improve run-time performance by from run-time costs to compile time"
msgstr "通过将运行时开销转移到编译时来提高运行时的性能"

#: ../../meta.rst:10
msgid "Simplify the development of Taichi standard library"
msgstr "简化Taichi标准库的开发"

#: ../../meta.rst:12
msgid ""
"Taichi kernels are *lazily instantiated* and a lot of computation can "
"happen at *compile-time*. Every kernel in Taichi is a template kernel, "
"even if it has no template arguments."
msgstr "Taichi内核是 *惰性实例化* 的，并且很多有计算可以发生在 *编译时*。即使没有模板参数，Taichi中的每一个内核也都是模板内核。"

#: ../../meta.rst:18
#, fuzzy
msgid "Template metaprogramming"
msgstr "元编程"

#: ../../meta.rst:20
msgid ""
"You may use ``ti.template()`` as a type hint to pass a tensor as an "
"argument. For example:"
msgstr ""

#: ../../meta.rst:23
msgid ""
"@ti.kernel\n"
"def copy(x: ti.template(), y: ti.template()):\n"
"    for i in x:\n"
"        y[i] = x[i]\n"
"\n"
"a = ti.var(ti.f32, 4)\n"
"b = ti.var(ti.f32, 4)\n"
"c = ti.var(ti.f32, 12)\n"
"d = ti.var(ti.f32, 12)\n"
"copy(a, b)\n"
"copy(c, d)"
msgstr ""

#: ../../meta.rst:38
msgid ""
"As shown in the example above, template programming may enable us to "
"reuse our code and provide more flexibility."
msgstr ""

#: ../../meta.rst:43
msgid "Dimensionality-independent programming using grouped indices"
msgstr "使用组合索引（grouped indices）的对维度不依赖的编程"

#: ../../meta.rst:45
msgid ""
"However, the ``copy`` template shown above is not perfect. For example, "
"it can only be used to copy 1D tensors. What if we want to copy 2D "
"tensors? Do we have to write another kernel?"
msgstr ""

#: ../../meta.rst:49
msgid ""
"@ti.kernel\n"
"def copy2d(x: ti.template(), y: ti.template()):\n"
"    for i, j in x:\n"
"        y[i, j] = x[i, j]"
msgstr ""

#: ../../meta.rst:56
msgid ""
"Not necessary! Taichi provides ``ti.grouped`` syntax which enables you to"
" pack loop indices into a grouped vector to unify kernels of different "
"dimensionalities. For example:"
msgstr ""

#: ../../meta.rst:60
msgid ""
"@ti.kernel\n"
"def copy(x: ti.template(), y: ti.template()):\n"
"    for I in ti.grouped(y):\n"
"        # I is a vector with same dimensionality with x and data type i32"
"\n"
"        # If y is 0D, then I = ti.Vector([]), which is equivalent to "
"`None` when used in x[I]\n"
"        # If y is 1D, then I = ti.Vector([i])\n"
"        # If y is 2D, then I = ti.Vector([i, j])\n"
"        # If y is 3D, then I = ti.Vector([i, j, k])\n"
"        # ...\n"
"        x[I] = y[I]\n"
"\n"
"@ti.kernel\n"
"def array_op(x: ti.template(), y: ti.template()):\n"
"    # if tensor x is 2D:\n"
"    for I in ti.grouped(x): # I is simply a 2D vector with data type i32\n"
"        y[I + ti.Vector([0, 1])] = I[0] + I[1]\n"
"\n"
"    # then it is equivalent to:\n"
"    for i, j in x:\n"
"        y[i, j + 1] = i + j"
msgstr ""

#: ../../meta.rst:85
msgid "Tensor metadata"
msgstr ""

#: ../../meta.rst:87
#, fuzzy
msgid ""
"Sometimes it is useful to get the data type (``tensor.dtype``) and shape "
"(``tensor.shape``) of tensors. These attributes can be accessed in both "
"Taichi- and Python-scopes."
msgstr ""
"有些时候获取张量的维度（ ``tensor.dim()`` ）和形状（ ``tensor.shape()`` "
"）是很有用的。这些函数既可以被应用在Taichi内核中，也可以被用在Python脚本中。"

#: ../../meta.rst:90
msgid ""
"@ti.func\n"
"def print_tensor_info(x: ti.template()):\n"
"  print('Tensor dimensionality is', len(x.shape))\n"
"  for i in ti.static(range(len(x.shape))):\n"
"    print('Size alone dimension', i, 'is', x.shape[i])\n"
"  ti.static_print('Tensor data type is', x.dtype)"
msgstr ""

#: ../../meta.rst:99
msgid "See :ref:`scalar_tensor` for more details."
msgstr ""

#: ../../meta.rst:103
msgid "For sparse tensors, the full domain shape will be returned."
msgstr "对稀疏张量而言，此处会返回其完整域的形状（full domain shape）。"

#: ../../meta.rst:107
msgid "Matrix & vector metadata"
msgstr ""

#: ../../meta.rst:109
msgid ""
"Getting the number of matrix columns and rows will allow you to write "
"dimensionality-independent code. For example, this can be used to unify "
"2D and 3D physical simulators."
msgstr ""

#: ../../meta.rst:113
msgid ""
"``matrix.m`` equals to the number of columns of a matrix, while "
"``matrix.n`` equals to the number of rows of a matrix. Since vectors are "
"considered as matrices with one column, ``vector.n`` is simply the "
"dimensionality of the vector."
msgstr ""

#: ../../meta.rst:118
msgid ""
"@ti.kernel\n"
"def foo():\n"
"  matrix = ti.Matrix([[1, 2], [3, 4], [5, 6]])\n"
"  print(matrix.n)  # 2\n"
"  print(matrix.m)  # 3\n"
"  vector = ti.Vector([7, 8, 9])\n"
"  print(vector.n)  # 3\n"
"  print(vector.m)  # 1"
msgstr ""

#: ../../meta.rst:132
msgid "Compile-time evaluations"
msgstr "编译时求值（Compile-time evaluations）"

#: ../../meta.rst:134
#, fuzzy
msgid ""
"Using compile-time evaluation will allow certain computations to happen "
"when kernels are being instantiated. This saves the overhead of those "
"computations at runtime."
msgstr "使用编译时求值可以使得某些计算在内核被实例化时发生。这些计算不会有运行时开支。"

#: ../../meta.rst:137
#, fuzzy
msgid ""
"Use ``ti.static`` for compile-time branching (for those who come from "
"C++17, this is `if constexpr "
"<https://en.cppreference.com/w/cpp/language/if>`_.):"
msgstr ""
"用 ``ti.static`` 来进行编译时的分支展开（对C++17的用户来说，这相当于是 `if constexpr "
"<https://en.cppreference.com/w/cpp/language/if>`_ ）"

#: ../../meta.rst:139
msgid ""
"enable_projection = True\n"
"\n"
"@ti.kernel\n"
"def static():\n"
"  if ti.static(enable_projection): # No runtime overhead\n"
"    x[0] = 1"
msgstr ""
"enable_projection = True\n"
"\n"
"@ti.kernel\n"
"def static():\n"
"  if ti.static(enable_projection): # 没有运行时开销\n"
"    x[0] = 1"

#: ../../meta.rst:149
#, fuzzy
msgid "Use ``ti.static`` for forced loop unrolling:"
msgstr "使用 ``ti.static`` 强制循环展开（forced loop unrolling）"

#: ../../meta.rst:151
msgid ""
"@ti.kernel\n"
"def func():\n"
"  for i in ti.static(range(4)):\n"
"      print(i)\n"
"\n"
"  # is equivalent to:\n"
"  print(0)\n"
"  print(1)\n"
"  print(2)\n"
"  print(3)"
msgstr ""

#: ../../meta.rst:166
msgid "When to use for loops with ``ti.static``"
msgstr "何时使用 ``ti.static`` 来进行for循环"

#: ../../meta.rst:168
msgid "There are several reasons why ``ti.static`` for loops should be used."
msgstr "下面有一些为何应该在for循环的同时使用 ``ti.static`` 的原因。"

#: ../../meta.rst:170
msgid "Loop unrolling for performance."
msgstr "循环展开以提高性能。"

#: ../../meta.rst:171
msgid ""
"Loop over vector/matrix elements. Indices into Taichi matrices must be a "
"compile-time constant. Indexing into taichi tensors can be run-time "
"variables. For example, if ``x`` is a 1-D tensor of 3D vector, accessed "
"as ``x[tensor_index][matrix index]``. The first index can be variable, "
"yet the second must be a constant."
msgstr ""
"对向量/矩阵的元素进行循环。矩阵的索引必须为编译时常量。张量的索引可以为运行时变量。例如，如果 ``x`` 是由3维向量组成的1维张量，并可以 "
"``x[tensor_index][matrix_index]`` "
"的形式访问。第一个索引（tensor_index）可以是变量，但是第二个索引（matrix_index）必须是一个常量。"

#: ../../meta.rst:173
msgid "For example, code for resetting this tensor of vectors should be"
msgstr "例如，向量张量（tensor of vectors）的重置代码应该为"

#: ../../meta.rst:175
#, fuzzy
msgid ""
"@ti.kernel\n"
"def reset():\n"
"  for i in x:\n"
"    for j in ti.static(range(x.n)):\n"
"      # The inner loop must be unrolled since j is a vector index instead"
"\n"
"      # of a global tensor index.\n"
"      x[i][j] = 0"
msgstr ""
"@ti.kernel\n"
"def reset():\n"
"  for i in x:\n"
"    for j in ti.static(range(3)):\n"
"      # 内层循环必须被展开，因为k是是向量的索引\n"
"      # 而不是全局的张量索引。\n"
"      x[i][j] = 0"

#~ msgid ""
#~ "@ti.kernel\n"
#~ "def copy(x: ti.template(), y: ti.template()):\n"
#~ "    for i in x:\n"
#~ "        y[i] = x[i]"
#~ msgstr ""

#~ msgid ""
#~ "@ti.kernel\n"
#~ "def copy(x: ti.template(), y: ti.template()):\n"
#~ "    for I in ti.grouped(y):\n"
#~ "        x[I] = y[I]\n"
#~ "\n"
#~ "@ti.kernel\n"
#~ "def array_op(x: ti.template(), y: ti.template()):\n"
#~ "    # If tensor x is 2D\n"
#~ "    for I in ti.grouped(x): # I"
#~ " is a vector of size x.dim() "
#~ "and data type i32\n"
#~ "        y[I + ti.Vector([0, 1])] = I[0] + I[1]\n"
#~ "    # is equivalent to\n"
#~ "    for i, j in x:\n"
#~ "        y[i, j + 1] = i + j"
#~ msgstr ""
#~ "@ti.kernel\n"
#~ "def copy(x: ti.template(), y: ti.template()):\n"
#~ "  for I in ti.grouped(y):\n"
#~ "    x[I] = y[I]\n"
#~ "\n"
#~ "@ti.kernel\n"
#~ "def array_op(x: ti.template(), y: ti.template()):\n"
#~ "  # 如果张量x是 2维的\n"
#~ "  for I in ti.grouped(x): # I 是一个大小为为x.dim()，类型为i32的向量\n"
#~ "    y[I + ti.Vector([0, 1])] = I[0] + I[1]\n"
#~ "  # 等价于\n"
#~ "  for i, j in x:\n"
#~ "    y[i, j + 1] = i + j"

#~ msgid "Tensor size reflection"
#~ msgstr "张量尺寸的反射（size reflection）"

#~ msgid ""
#~ "@ti.func\n"
#~ "def print_tensor_size(x: ti.template()):\n"
#~ "  print(x.dim())\n"
#~ "  for i in ti.static(range(x.dim())):\n"
#~ "    print(x.shape()[i])"
#~ msgstr ""

#~ msgid ""
#~ "@ti.kernel\n"
#~ "def g2p(f: ti.i32):\n"
#~ "for p in range(0, n_particles):\n"
#~ " base = ti.cast(x[f, p] * inv_dx - 0.5, ti.i32)\n"
#~ " fx = x[f, p] * inv_dx - ti.cast(base, real)\n"
#~ " w = [0.5 * ti.sqr(1.5 - fx), 0.75 - ti.sqr(fx - 1.0),\n"
#~ "      0.5 * ti.sqr(fx - 0.5)]\n"
#~ " new_v = ti.Vector([0.0, 0.0])\n"
#~ " new_C = ti.Matrix([[0.0, 0.0], [0.0, 0.0]])\n"
#~ "\n"
#~ " # Unrolled 9 iterations for higher performance\n"
#~ " for i in ti.static(range(3)):\n"
#~ "   for j in ti.static(range(3)):\n"
#~ "     dpos = ti.cast(ti.Vector([i, j]), real) - fx\n"
#~ "     g_v = grid_v_out[base(0) + i, base(1) + j]\n"
#~ "     weight = w[i](0) * w[j](1)\n"
#~ "     new_v += weight * g_v\n"
#~ "     new_C += 4 * weight * ti.outer_product(g_v, dpos) * inv_dx\n"
#~ "\n"
#~ " v[f + 1, p] = new_v\n"
#~ " x[f + 1, p] = x[f, p] + dt * v[f + 1, p]\n"
#~ " C[f + 1, p] = new_C"
#~ msgstr ""
#~ "@ti.kernel\n"
#~ "def g2p(f: ti.i32):\n"
#~ "for p in range(0, n_particles):\n"
#~ " base = ti.cast(x[f, p] * inv_dx - 0.5, ti.i32)\n"
#~ " fx = x[f, p] * inv_dx - ti.cast(base, real)\n"
#~ " w = [0.5 * ti.sqr(1.5 - fx), 0.75 - ti.sqr(fx - 1.0),\n"
#~ "      0.5 * ti.sqr(fx - 0.5)]\n"
#~ " new_v = ti.Vector([0.0, 0.0])\n"
#~ " new_C = ti.Matrix([[0.0, 0.0], [0.0, 0.0]])\n"
#~ "\n"
#~ " # 展开了9次迭代来获得更高的性能\n"
#~ " for i in ti.static(range(3)):\n"
#~ "   for j in ti.static(range(3)):\n"
#~ "     dpos = ti.cast(ti.Vector([i, j]), real) - fx\n"
#~ "     g_v = grid_v_out[base(0) + i, base(1) + j]\n"
#~ "     weight = w[i](0) * w[j](1)\n"
#~ "     new_v += weight * g_v\n"
#~ "     new_C += 4 * weight * ti.outer_product(g_v, dpos) * inv_dx\n"
#~ "\n"
#~ " v[f + 1, p] = new_v\n"
#~ " x[f + 1, p] = x[f, p] + dt * v[f + 1, p]\n"
#~ " C[f + 1, p] = new_C"

