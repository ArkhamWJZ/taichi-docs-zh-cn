# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018, Yuanming Hu
# This file is distributed under the same license as the taichi package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: taichi 0.5.14\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-04-24 11:24+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../compilation.rst:4
msgid "The life of a Taichi kernel"
msgstr ""

#: ../../compilation.rst:6
msgid ""
"Sometimes it is helpful to understand the life cycle of a Taichi kernel. "
"In short, compilation will only happen on the first invocation of an "
"instance of a kernel."
msgstr ""

#: ../../compilation.rst:9
msgid "Life cycle of a Taichi kernel looks like this:"
msgstr ""

#: ../../compilation.rst:11 ../../compilation.rst:38
msgid "Kernel registration"
msgstr ""

#: ../../compilation.rst:12 ../../compilation.rst:45
msgid "Template instantiation and caching"
msgstr ""

#: ../../compilation.rst:13
msgid "Python AST transforms"
msgstr ""

#: ../../compilation.rst:14
msgid "Taichi IR compilation, optimization, and binary generation"
msgstr ""

#: ../../compilation.rst:15
msgid "Launching"
msgstr ""

#: ../../compilation.rst:19
msgid "Let's consider the following simple kernel:"
msgstr ""

#: ../../compilation.rst:21
msgid ""
"@ti.kernel\n"
"def add(tensor: ti.template(), delta: ti.i32):\n"
"  for i in tensor:\n"
"    tensor[i] += delta"
msgstr ""

#: ../../compilation.rst:29
msgid "We also allocate two 1D tensors to simplify discussion:"
msgstr ""

#: ../../compilation.rst:31
msgid ""
"x = ti.var(dt=ti.f32, shape=128)\n"
"y = ti.var(dt=ti.f32, shape=16)"
msgstr ""

#: ../../compilation.rst:39
msgid ""
"When the ``ti.kernel`` decorator is executed, a kernel named ``add`` is "
"registered. Specifically, the Python Abstract Syntax Tree (AST) of the "
"``add`` function will be memorized. No compilation will happen until the "
"first invocation of ``add``."
msgstr ""

#: ../../compilation.rst:47
msgid "add(x, 42)"
msgstr ""

#: ../../compilation.rst:51
msgid ""
"When ``add`` is called for the first time, the Taichi frontend compiler "
"will instantiate the kernel."
msgstr ""

#: ../../compilation.rst:53
msgid ""
"When you have a second call with the same template signature (explained "
"later), e.g.,"
msgstr ""

#: ../../compilation.rst:55
msgid "add(x, 1)"
msgstr ""

#: ../../compilation.rst:59
msgid "Taichi will directly reuse the previously compiled binary."
msgstr ""

#: ../../compilation.rst:61
msgid ""
"Arguments hinted with ``ti.template()`` are template arguments, and will "
"incur template instantiation. For example,"
msgstr ""

#: ../../compilation.rst:63
msgid "add(y, 42)"
msgstr ""

#: ../../compilation.rst:67
msgid "will lead to a new instantiation of **add**."
msgstr ""

#: ../../compilation.rst:70
msgid ""
"**Template signatures** are what distinguish different instantiations of "
"a kernel template. The signature of ``add(x, 42)`` is ``(x, ti.i32)``, "
"which is the same as that of ``add(x, 1)``. Therefore, the latter can "
"reuse the previously compiled binary. The signature of ``add(y, 42)`` is "
"``(y, ti.i32)``, a different value from the previous signature, therefore"
" a new instantiation and compilation will happen."
msgstr ""

#: ../../compilation.rst:76
msgid ""
"Many basic operations in the Taichi standard library is implemented using"
" Taichi kernels for performance, with more or less metaprogramming "
"tricks. Invoking them will incur **implicit kernel instantiations**"
msgstr ""

#: ../../compilation.rst:79
msgid ""
"Examples include ``x.to_numpy()`` and ``y.from_torch(torch_tensor)``. "
"When you invoke these functions, you will see kernel instantiations, as "
"Taichi kernels will be generated to offload the hard work to multiple CPU"
" cores/GPUs."
msgstr ""

#: ../../compilation.rst:82
msgid ""
"As mentioned before, the second time you call the same operation, the "
"cached compiled kernel will be reused and no further compilation is "
"needed."
msgstr ""

#: ../../compilation.rst:85
msgid "Code transformation and optimizations"
msgstr ""

#: ../../compilation.rst:87
msgid ""
"When a new instantiation happens, the Taichi frontend compiler will "
"transform the kernel body AST into a Python script, which, when executed,"
" emits a Taichi frontend AST. Basically, some patches are applied to the "
"Python AST so that the Taichi frontend can recognize it."
msgstr ""

#: ../../compilation.rst:91
msgid ""
"The Taichi AST lowering pass translates Taichi frontend IR into "
"hierarchical static single assignment (SSA) IR, which allows a series of "
"further IR passes to happen, such as"
msgstr ""

#: ../../compilation.rst:94
msgid "Loop vectorization"
msgstr ""

#: ../../compilation.rst:95
msgid "Type inference and checking"
msgstr ""

#: ../../compilation.rst:96
msgid ""
"General simplifications such as common subexpression elimination (CSE), "
"dead instruction elimination (DIE), constant folding, and store "
"forwarding"
msgstr ""

#: ../../compilation.rst:97
msgid "Access lowering"
msgstr ""

#: ../../compilation.rst:98
msgid "Data access optimizations"
msgstr ""

#: ../../compilation.rst:99
msgid ""
"Reverse-mode automatic differentiation (if using differentiable "
"programming)"
msgstr ""

#: ../../compilation.rst:100
msgid "Parallelization and offloading"
msgstr ""

#: ../../compilation.rst:101
msgid "Atomic operation demotion"
msgstr ""

#: ../../compilation.rst:104
msgid "The just-in-time (JIT) compilation engine"
msgstr ""

#: ../../compilation.rst:106
msgid ""
"Finally, the optimized SSA IR is fed into the LLVM IR codegen, and LLVM "
"JIT generates high-performance executable CPU/GPU programs."
msgstr ""

#: ../../compilation.rst:109
msgid "Kernel launching"
msgstr ""

#: ../../compilation.rst:111
msgid ""
"Taichi kernels will be ultimately launched as multi-threaded CPU tasks or"
" CUDA kernels."
msgstr ""

